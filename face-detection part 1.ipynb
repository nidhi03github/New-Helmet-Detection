{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c145257f431eea690a59385a2f1cde82f7409924"
   },
   "source": [
    "# Face recognition with Tensorflow Object Detection API (LFW dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named cv2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2e7705de5544>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#image processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#extracting zippped file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named cv2"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "#Visiulazation\n",
    "import matplotlib.pyplot as plt\n",
    "#image processing\n",
    "import cv2\n",
    "#extracting zippped file\n",
    "import tarfile\n",
    "#systems\n",
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c6f7fbef29d879ec925a743c0c6fcaeac50865c"
   },
   "source": [
    "This is the complete code for part 1 of [my Medium article](https://medium.com/@saidakbarp/real-time-face-recognition-tflite-3fb818ac039a). The complete notebook for the part 2 of my Medium article is [here](https://www.kaggle.com/saidakbarp/face-recognition-part-2/notebook).\n",
    "\n",
    "Face recognition relies on the dataset that has been annotated with boxes. Manually annotating faces in each images can be time consuming and for large scale training, manual annotation is impractical. For this reason we will use available face annotation tools to annotate each image with boxes. Afterwards, we can move to Object detection training part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "12983332defabe2f3e6ba4b312fb717649e4472b"
   },
   "outputs": [],
   "source": [
    "#example\n",
    "imgg=\"/kaggle/input/photos/ben.jpg\"\n",
    "celeb=cv2.imread(imgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b395e885e4c4c963557d11f6ba67e9612e189225"
   },
   "outputs": [],
   "source": [
    "def show_image(image):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    #Before showing image, bgr color order transformed to rgb order\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8fd0dcafc3951e089ca1c1b41e941ed0f8e1aaba"
   },
   "outputs": [],
   "source": [
    "show_image(celeb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7e9517349c29c7806b3ff312c1af45aad871b269"
   },
   "outputs": [],
   "source": [
    "# Our face detection function that uses haarcascade from OpenCV\n",
    "def face_detection(img):\n",
    "    face_cascade = cv2.CascadeClassifier('/kaggle/input/haarcascades/haarcascade_frontalface_alt.xml')\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    print('Number of faces detected:', len(faces))\n",
    "        \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        #img = img[y:y+h, x:x+w] # for cropping\n",
    "    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return cv_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9018f1f132efe13cc497df1bf1bb1beaaea8469"
   },
   "outputs": [],
   "source": [
    "#imgg2=cv2.imread(\"/kaggle/input/photos/ben.jpg\")\n",
    "a=face_detection(celeb)\n",
    "plt.imshow(a)\n",
    "plt.show() \n",
    "\n",
    "# as shown below, the library is not detecting this particular face angle of Ben Afflek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c76defb5172473950560d19e01ba89883e8cd2ec"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,18))\n",
    "img=cv2.imread(\"../input/photos/people.jpg\")\n",
    "c=face_detection(img)\n",
    "plt.imshow(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "424c50a14001ab11586a59c93b0b46caf3b4a9c6"
   },
   "source": [
    "Haarcascade has a con: it can not detect non-frontal face images and also boxes sometimes do not include full face, clipping chins or forehead. Let us try a better model: openCV DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "86ded388129c9caeb396d6439f5b10f0cdcce6f3"
   },
   "outputs": [],
   "source": [
    "# using openCV DNN\n",
    "# load the model\n",
    "#modelFile = \"../input/opencv-dnn/opencv_face_detector_uint8.pb\" \n",
    "#configFile = \"../input/opencv-dnn/opencv_face_detector.pbtxt\"\n",
    "#net = cv2.dnn.readNetFromTensorflow(modelFile, configFile)\n",
    "\n",
    "modelFile =\"../input/opencvdnnfp16/res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "configFile = \"../input/opencvdnnfp16/deploy.prototxt\"\n",
    "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff055854fdc750b13176abc4443a626774176ec6"
   },
   "outputs": [],
   "source": [
    "#function to extract box dimensions\n",
    "def face_dnn(img, coord=False):\n",
    "    blob = cv2.dnn.blobFromImage(img, 1, (224,224), [104, 117, 123], False, False) #\n",
    "    # params: source, scale=1, size=300,300, mean RGB values (r,g,b), rgb swapping=false, crop = false\n",
    "    conf_threshold=0.8 # confidence at least 60%\n",
    "    frameWidth=img.shape[1] # get image width\n",
    "    frameHeight=img.shape[0] # get image height\n",
    "    max_confidence=0\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    detection_index=0\n",
    "    bboxes = []\n",
    "    \n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            \n",
    "            if max_confidence < confidence: # only show maximum confidence face\n",
    "                max_confidence = confidence\n",
    "                detection_index = i\n",
    "    i=detection_index        \n",
    "    x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
    "    y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
    "    x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
    "    y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
    "    cv2.rectangle(img,(x1,y1),(x2,y2),(255,255,0),2)\n",
    "    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    if coord==True:\n",
    "        return x1, y1, x2, y2\n",
    "    return cv_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f244605b1f3a92d48755c8ff87b1b87c3a59c0f0"
   },
   "outputs": [],
   "source": [
    "#gray=cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "# multiple faces needs increasing the size of image as well as multiple detections\n",
    "def nfaces_dnn(img):\n",
    "    blob = cv2.dnn.blobFromImage(img, 1.2, (1200,1200), [104, 117, 123], False, False) #\n",
    "    # params: source, scale=1, size=300,300, mean RGB values (r,g,b), rgb swapping=false, crop = false\n",
    "    conf_threshold=0.6 # confidence at least 60%\n",
    "    frameWidth=img.shape[1] # get image width\n",
    "    frameHeight=img.shape[0] # get image height\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "  \n",
    "    bboxes = []\n",
    "    \n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "                  \n",
    "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
    "            cv2.rectangle(img,(x1,y1),(x2,y2),(255,255,0),2)\n",
    "    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return cv_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "769167b85005e67201d53cb5a2f63ca87f40d1a5"
   },
   "outputs": [],
   "source": [
    "a=face_dnn(celeb)\n",
    "plt.imshow(a)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b4c0298e48b104d3b57caa34c6a5139d9730fe0"
   },
   "outputs": [],
   "source": [
    "img=cv2.imread(\"../input/photos/people.jpg\")\n",
    "c=nfaces_dnn(img)\n",
    "plt.figure(figsize=(15,18))\n",
    "plt.imshow(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db3cf1a12c55c40dc904a445f9c107afae13d33c"
   },
   "source": [
    "As output shows, although 'multiple faces' function does not register some faces from a group of people, DNN is able to detect non-frontal faces without clipping chin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fa578d0a48bdbeb5b8f8dbd9536b33ddf17fafc8"
   },
   "source": [
    "Our main purpose is to use LFW people images that have at least 20 examples ( later, obtain 100 images of top 100 celebrities from Google image search), then crop their faces (single face photos). Afterwards, we train our mobilenetV2 coco model to recognize those celebrities. Final goal is to identify celebrities from their photos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9c4fa59090210b1023ab7ab92e178fa45672b84"
   },
   "source": [
    "## Now we extract LFW for face detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b633fad9312775ef5c8229f158342d7569d6ba1"
   },
   "source": [
    "Each picture is centered on a single face. The typical task is called Face Verification: given a pair of two pictures, a binary classifier must predict whether the two images are from the same person.\n",
    "\n",
    "An alternative task, Face Recognition or Face Identification is: given the picture of the face of an unknown person, identify the name of the person by referring to a gallery of previously seen pictures of identified persons.\n",
    "\n",
    "Both Face Verification and Face Recognition are tasks that are typically performed on the output of a model trained to perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the OpenCV library. The LFW faces were extracted by this face detector from various online websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bfc8f80a12b3a38b84659236865ef6b582308fbd"
   },
   "outputs": [],
   "source": [
    "#os.listdir('../input/lfwpeople/')\n",
    "fname='../input/lfwpeople/lfwfunneled.tgz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e10bafc27840d3744dddfecb1ed6856c7d7888e"
   },
   "outputs": [],
   "source": [
    "def jpg_files(members): #only extract jpg files\n",
    "    for tarinfo in members:\n",
    "        if os.path.splitext(tarinfo.name)[1] == \".jpg\":\n",
    "            yield tarinfo\n",
    "def untar(fname,path=\"LFW\"): #untarring the archive\n",
    "    tar = tarfile.open(fname)\n",
    "    tar.extractall(path,members=jpg_files(tar))\n",
    "    tar.close()\n",
    "    if path is \"\":\n",
    "        print(\"File Extracted in Current Directory\")\n",
    "    else:\n",
    "        print(\"File Extracted in to\",  path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c4a45432dd47e724a4cb86043658fe74d23014f0"
   },
   "outputs": [],
   "source": [
    "untar(fname,\"LFW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7436a1ad5ca7b4bba079d81712ea12bca2d0e0d6"
   },
   "outputs": [],
   "source": [
    "len(os.listdir('../working/LFW/lfw_funneled/')) # total number of folders (people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d17df96e44b0d2df41dfb11dbfe139598b605c62"
   },
   "outputs": [],
   "source": [
    "# total number of images\n",
    "total = sum([len(files) for r, d, files in os.walk('../working/LFW/lfw_funneled/')])\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b2661b21895fd41a209e9bd0ebc9cc346e5713b5"
   },
   "outputs": [],
   "source": [
    "count=0 \n",
    "imglist=[]\n",
    "# \n",
    "for r, d, files in os.walk('../working/LFW/lfw_funneled/'):   \n",
    "    if len(files)>=20: \n",
    "        imglist.append(r)\n",
    "        #print(count, r)\n",
    "        count+=1 # counts how many folders have with at least 20 images\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "252eda3e435f4762f0f1ebcb0f13e73723daf0e6"
   },
   "outputs": [],
   "source": [
    "#os.listdir(imglist[0])\n",
    "# pick one random photo\n",
    "a=np.random.randint(0,20)\n",
    "b=np.random.randint(0,62)\n",
    "imglist[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd52c8a1fd8b4e5564b7c86532df61e849cbb543"
   },
   "outputs": [],
   "source": [
    "# show the random photo with face detected\n",
    "img=imglist[b]+'/'+os.listdir(imglist[b])[a]\n",
    "img=cv2.imread(img)\n",
    "c=face_dnn(img)\n",
    "plt.imshow(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d912b24e81163c53c6e01976634a72711474892d"
   },
   "source": [
    "We have a list of people (62) with at least 20 images. Now, we will use this annotation of faces, and train MobileNet NN model to recognize those people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b798334765de49f1ed1f3581f9e1461b9b9df65"
   },
   "source": [
    "## Creating training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0fa5af11a548f24f4ae0b6838bd8eda53e4473de"
   },
   "outputs": [],
   "source": [
    "#remove unused folders\n",
    "import shutil\n",
    "pathd='../working/LFW/lfw_funneled/'\n",
    "#shutil.rmtree(os.path.realpath('LFW'))\n",
    "for dirs in os.listdir(pathd):\n",
    "    if not (pathd+dirs) in imglist:\n",
    "        shutil.rmtree(os.path.realpath(pathd+dirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94780f9233102a93bb7bbba73a46d3ee8038f325"
   },
   "outputs": [],
   "source": [
    "dirs=os.listdir(pathd)\n",
    "dirs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52d9a6bd8a88163bcd1cc66add76f0145668fa65"
   },
   "outputs": [],
   "source": [
    "#example of box coordinates\n",
    "#a=np.random.randint(0,20)\n",
    "b=np.random.randint(0,62)\n",
    "for img in os.listdir(pathd+dirs[b])[:5]:\n",
    "    #print(pathd+dirs[0]+'/'+img)\n",
    "    print(dirs[b])\n",
    "    img=cv2.imread(pathd+dirs[b]+'/'+img)\n",
    "    x1, y1, x2, y2=face_dnn(img, True)\n",
    "    #print coordinates of the detected face\n",
    "    print(x1, y1, x2, y2)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0abc746b3dc6fc47117f08a28fed9fa65c86431"
   },
   "outputs": [],
   "source": [
    "os.listdir('../working/LFW/')\n",
    "pathd\n",
    "#(os.listdir(pathd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "37208ea1dd47032adf7a3ff5615a8f65bf1d1c6f"
   },
   "outputs": [],
   "source": [
    "# # creating test and train set\n",
    "from numpy import random\n",
    "datadir='../working/LFW/'\n",
    "train=datadir+'train/'\n",
    "test=datadir+'test/'\n",
    "\n",
    "if not os.path.exists(train):\n",
    "    os.mkdir(train)\n",
    "if not os.path.exists(test):\n",
    "    os.mkdir(test)\n",
    "    \n",
    "for dirs in os.listdir(pathd):\n",
    "    filenames = os.listdir(pathd+dirs)\n",
    "    filenames.sort()  # make sure that the filenames have a fixed order before shuffling\n",
    "    random.seed(402)\n",
    "    random.shuffle(filenames) # shuffles the ordering of filenames (deterministic given the chosen seed)\n",
    "    split = int(0.85 * len(filenames))\n",
    "    train_filenames = filenames[:split] # splitting filenames into two parts\n",
    "    test_filenames = filenames[split:]\n",
    "    for img in train_filenames:\n",
    "        full_file_name = os.path.join(pathd+dirs, img)\n",
    "        cur_dir=os.path.join(train+dirs)\n",
    "        #print(cur_dir)\n",
    "        if not os.path.exists(cur_dir): # create this current person's folder for training\n",
    "            os.mkdir(cur_dir)\n",
    "        shutil.copy(full_file_name, cur_dir)\n",
    "    for img in test_filenames:\n",
    "        full_file_name = os.path.join(pathd+dirs, img)\n",
    "        cur_dir=os.path.join(test+dirs)\n",
    "        if not os.path.exists(cur_dir): # create this current person's folder for testing\n",
    "            os.mkdir(cur_dir)\n",
    "        shutil.copy(full_file_name, cur_dir)\n",
    "        #a=full_file_name+' '+test+dirs\n",
    "shutil.rmtree('../working/LFW/lfw_funneled/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa66ba219c5d15bb5110479d11b10d0c2ea19049"
   },
   "outputs": [],
   "source": [
    "# total number of images left\n",
    "total = sum([len(files) for r, d, files in os.walk(datadir)])\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f6ef8a35624b61d11138e4f128b873bf5bda2413"
   },
   "outputs": [],
   "source": [
    "labeldir=\"Labels/\" # labels dir\n",
    "wdir=\"../working/LFW/\"\n",
    "lab=wdir+labeldir\n",
    "if not os.path.exists(lab):\n",
    "    os.mkdir(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "805ec0f78e02b90497339e39239e09be8c631f8f"
   },
   "outputs": [],
   "source": [
    "# function for creating box labels as txt file\n",
    "def label_txt(pathdr, lab_dir):\n",
    "    for fol in os.listdir(pathdr):\n",
    "        tfile = open(lab_dir+fol+\".txt\",\"w+\")\n",
    "        for img in os.listdir(pathdr+fol):\n",
    "            pathimg=os.path.join(pathdr+fol, img)\n",
    "            #print(pathimg)\n",
    "            pic=cv2.imread(pathimg)\n",
    "            x1, y1, x2, y2=face_dnn(pic, True) # face detection and then saving into txt file       \n",
    "            tfile.write(img+' '+str(x1)+' '+str(x2)+' '+str(y1)+' '+str(y2)+'\\n')          \n",
    "        tfile.close()\n",
    "    print('Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9d61403d79f8dc5c358570dab213f3fc890cdbe5"
   },
   "outputs": [],
   "source": [
    "lab_dir=lab+'train/'\n",
    "os.mkdir(lab_dir)\n",
    "label_txt(train, lab_dir)\n",
    "\n",
    "lab_dir=lab+'test/'\n",
    "os.mkdir(lab_dir)\n",
    "label_txt(test, lab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b86f87860d85c38bc6156197be2085cf939f26e1"
   },
   "outputs": [],
   "source": [
    "#let's check if txt files are correct:\n",
    "f2 = open(\"../working/LFW/Labels/test/Arnold_Schwarzenegger.txt\",\"r\")\n",
    "print(f2.read())\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f4bfb31b07dc9d8a3aed66c7f24cb70523be108"
   },
   "outputs": [],
   "source": [
    "# checking Arnold_Schwarzenegger_0041.jpg\n",
    "pic=cv2.imread(test+'Arnold_Schwarzenegger/Arnold_Schwarzenegger_0041.jpg')\n",
    "cv2.rectangle(pic,(77 ,62),(176,194),(255,255,0),2)\n",
    "plt.imshow(pic)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "762ab23282a317bbd96c7107e8c9ea0d319dd383"
   },
   "source": [
    "As we can see box annotation is correct! Now, we have to save annotations as tfrecord:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9cb6593b281ed0b933a7988799b42ea34894749e"
   },
   "source": [
    "# Creating tfrecords with annotated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7dfd5ae7d62f7e11e299ac7a13483e28181bdd88"
   },
   "outputs": [],
   "source": [
    "def read_txt(person, photo):  \n",
    "    txtfile = labels+person+\".txt\"\n",
    "    txtfile_contents = open(txtfile, \"r\")\n",
    "    txtlines = txtfile_contents.readlines()\n",
    "    txtfile_contents.close()\n",
    "    for line in txtlines:\n",
    "        if photo in line:\n",
    "            txtlines=line\n",
    "    return txtlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "93064ab42a5d8eaf17a58ef32135e4cba8004670"
   },
   "outputs": [],
   "source": [
    "!pip install object_detection >obj_dtc.txt # installing object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52983db42070595ada97632d3df69305847f1016"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from object_detection.utils import dataset_util\n",
    "\n",
    "# flags = tf.app.flags\n",
    "# flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\n",
    "# FLAGS = flags.FLAGS\n",
    "# modified from source: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md\n",
    "def create_tf_example(photo, person, iclass, foldr):\n",
    "    # one image at a time\n",
    "    img_f=os.path.join(foldr+person,photo+\".jpg\")\n",
    "    pic = Image.open(img_f)\n",
    "    height = pic.height # Image height\n",
    "    width = pic.width # Image width\n",
    "    filename = str.encode(photo) # Filename of the image. Empty if image is not from file\n",
    "    #encoded_image_data = None # Encoded image bytes\n",
    "    image_data = tf.gfile.GFile(img_f,'rb').read()\n",
    "    \n",
    "    image_format = b'jpeg' #None #  or b'png'\n",
    "    #declare coordinates\n",
    "    xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n",
    "    xmaxs = [] # List of normalized right x coordinates in bounding box\n",
    "             # (1 per box)\n",
    "    ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n",
    "    ymaxs = [] # List of normalized bottom y coordinates in bounding box\n",
    "             # (1 per box)\n",
    "    classes_text = [] # List of string class name of bounding box (1 per box)\n",
    "    classes = [] # List of integer class id of bounding box (1 per box)\n",
    "    \n",
    "    txtlines = read_txt(person, photo)\n",
    "\n",
    "    labels = txtlines.split()\n",
    "\n",
    "    xmins.append(float(labels[1])/width)\n",
    "    xmaxs.append(float(labels[2])/width)\n",
    "    ymins.append(float(labels[3])/height)\n",
    "    ymaxs.append(float(labels[4])/height)\n",
    "\n",
    "    classes_text.append(str.encode(person))\n",
    "    classes.append(iclass) #### iterator is needed\n",
    "    #print(xmins, xmaxs, ymins, ymaxs, classes_text, photo, img_f) # for test purposes\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "      'image/height': dataset_util.int64_feature(height),\n",
    "      'image/width': dataset_util.int64_feature(width),\n",
    "      'image/filename': dataset_util.bytes_feature(filename),\n",
    "      'image/source_id': dataset_util.bytes_feature(filename),\n",
    "      'image/encoded': dataset_util.bytes_feature(image_data),\n",
    "      'image/format': dataset_util.bytes_feature(image_format),\n",
    "      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "      'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e58bf9013dc22e119667b7279fbcd61ec21d109e"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#saving tfrecords\n",
    "def save_tf(folder):\n",
    "    tf_file=folder.split('/')[-2] +'.tfrecord'\n",
    "    writer = tf.python_io.TFRecordWriter('../working/'+tf_file)\n",
    "    \n",
    "    labelmap = '../working/'+'object_label.pbtxt' # for model training\n",
    "    txtf = open(labelmap, \"w\")\n",
    "    \n",
    "    labels = '../working/'+'labels.txt' # for android deployment\n",
    "    txtl = open(labels, \"w\")\n",
    "    \n",
    "    for ind, person in enumerate(os.listdir(folder)):\n",
    "        iclass=ind+1\n",
    "        txtf.write(\"item\\n{\\n  id: %s\\n  name: '%s'\\n}\\n\"%(iclass,person))\n",
    "        txtl.write(\"%s\\n\"%person)\n",
    "        #print(iclass, person)\n",
    "        for photo in os.listdir(folder+person):\n",
    "            tf_example = create_tf_example(photo.split('.')[0], person, iclass, folder) #004.jpg, arnold, 1\n",
    "            #print('Folder:', pathd+fol, iclass)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "    txtf.close()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a8c8ea115ca2c9c1403e004f8c0e1254cc24bd80"
   },
   "outputs": [],
   "source": [
    "labels='../working/LFW/Labels/train/'\n",
    "save_tf(train)\n",
    "\n",
    "labels='../working/LFW/Labels/test/'\n",
    "save_tf(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "09388859024fe9131f83892ef1a01c13c6078a5b"
   },
   "outputs": [],
   "source": [
    "os.stat('../working/train.tfrecord').st_size/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f86c6e6575ee32d80288d92285737034971713d9"
   },
   "outputs": [],
   "source": [
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "make_tarfile('test_images.tar.gz','/kaggle/working/LFW/test')        \n",
    "shutil.rmtree('/kaggle/working/LFW/')\n",
    "#os.listdir('/kaggle/working/LFW/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5fb4f24a40dfebec5d110f1c8395c8a92701364"
   },
   "outputs": [],
   "source": [
    "os.listdir('/kaggle/working/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7314d592fef766fbb9ca164c5b98a1ad07b0803c"
   },
   "outputs": [],
   "source": [
    "# for test purposes\n",
    "# pic = Image.open('../working/LFW/test/Jennifer_Capriati/Jennifer_Capriati_0007.jpg')\n",
    "# height = pic.height # Image height\n",
    "# width = pic.width # Image width\n",
    "# pic=cv2.imread('../working/LFW/test/Jennifer_Capriati/Jennifer_Capriati_0007.jpg')\n",
    "# cv2.rectangle(pic,(int(0.304*width),int(0.236*height)),(int(0.692*width),int(0.768*height)),(255,255,0),2)\n",
    "# plt.imshow(pic)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "201b56f389b3110643eb29cf7ccdb11ce885b483"
   },
   "source": [
    "Our tfrecords and labels are ready for model training. [Part 2](https://www.kaggle.com/saidakbarp/face-recognition-part-2/notebook) shows how to train an existing model and convert it to tflite directly in jupyter notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
